"""
éŸ³é¢‘æœåŠ¡æ¨¡å— - ç»Ÿä¸€ç®¡ç†ASRå’ŒTTSåŠŸèƒ½
æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼ï¼Œæä¾›æµå¼å’Œæ‰¹é‡å¤„ç†æ¥å£
"""

import os
import base64
import io
import tempfile
import requests
from typing import Optional, Dict, Any, Union, AsyncGenerator
import numpy as np
import soundfile as sf
from pydub import AudioSegment
from openai import OpenAI
import asyncio

from config import Config


class AudioService:
    """éŸ³é¢‘å¤„ç†æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–éŸ³é¢‘æœåŠ¡"""
        self.api_key = Config.DASHSCOPE_API_KEY
        self.sample_rate = Config.SAMPLE_RATE
        self.channels = Config.CHANNELS
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if Config.USE_OPENAI_TTS:
            self.openai_client = OpenAI(api_key=Config.OPENAI_API_KEY)
            print(f"âœ… OpenAI TTSå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        
        # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
        self.supported_formats = ['wav', 'mp3', 'm4a', 'flac', 'ogg']
        
        print(f"âœ… éŸ³é¢‘æœåŠ¡åˆå§‹åŒ–å®Œæˆ - é‡‡æ ·ç‡: {self.sample_rate}, å£°é“: {self.channels}")
    
    def convert_audio_format(
        self, 
        audio_data: bytes, 
        source_format: str, 
        target_format: str = 'wav'
    ) -> bytes:
        """
        è½¬æ¢éŸ³é¢‘æ ¼å¼
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘æ•°æ®
            source_format: æºæ ¼å¼
            target_format: ç›®æ ‡æ ¼å¼
            
        Returns:
            è½¬æ¢åçš„éŸ³é¢‘æ•°æ®
        """
        try:
            # ä½¿ç”¨pydubè¿›è¡Œæ ¼å¼è½¬æ¢
            audio = AudioSegment.from_file(
                io.BytesIO(audio_data), 
                format=source_format
            )
            
            # ç»Ÿä¸€é‡‡æ ·ç‡å’Œå£°é“
            audio = audio.set_frame_rate(self.sample_rate)
            audio = audio.set_channels(self.channels)
            
            # å¯¼å‡ºä¸ºç›®æ ‡æ ¼å¼
            output_buffer = io.BytesIO()
            audio.export(output_buffer, format=target_format)
            
            return output_buffer.getvalue()
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            raise
    
    def prepare_audio_for_asr(self, audio_data: bytes, source_format: str = 'wav') -> str:
        """
        ä¸ºASRå‡†å¤‡éŸ³é¢‘æ•°æ®
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            source_format: æºæ ¼å¼
            
        Returns:
            base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
        """
        try:
            # è½¬æ¢ä¸ºwavæ ¼å¼
            if source_format != 'wav':
                audio_data = self.convert_audio_format(audio_data, source_format, 'wav')
            
            # base64ç¼–ç 
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            return audio_base64
            
        except Exception as e:
            print(f"âŒ ASRéŸ³é¢‘å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    async def speech_to_text(
        self, 
        audio_data: Union[bytes, str], 
        source_format: str = 'wav',
        language: str = 'zh'
    ) -> Dict[str, Any]:
        """
        è¯­éŸ³è½¬æ–‡å­— (ASR)
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®ï¼ˆbytesï¼‰æˆ–base64å­—ç¬¦ä¸²
            source_format: éŸ³é¢‘æ ¼å¼
            language: è¯­è¨€ä»£ç 
            
        Returns:
            è¯†åˆ«ç»“æœ
        """
        try:
            # å¤„ç†éŸ³é¢‘æ•°æ®
            if isinstance(audio_data, bytes):
                audio_base64 = self.prepare_audio_for_asr(audio_data, source_format)
            else:
                audio_base64 = audio_data
            
            # æ„å»ºASRè¯·æ±‚
            messages = [
                {
                    "role": "system",
                    "content": [{"text": ""}]
                },
                {
                    "role": "user",
                    "content": [
                        {"audio": f"data:audio/wav;base64,{audio_base64}"},
                    ]
                }
            ]
            
            # è°ƒç”¨DashScope ASR API
            response = dashscope.MultiModalConversation.call(
                api_key=self.api_key,
                model=Config.ASR_MODEL,
                messages=messages,
                result_format="message",
                asr_options={
                    "enable_lid": True,  # è¯­è¨€è¯†åˆ«
                    "enable_itn": True   # æ•°å­—è½¬æ¢
                }
            )
            
            if response.status_code == 200:
                # æå–è¯†åˆ«ç»“æœ
                result_text = ""
                if hasattr(response, 'output') and hasattr(response.output, 'choices'):
                    if len(response.output.choices) > 0:
                        choice = response.output.choices[0]
                        if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                            result_text = choice.message.content
                
                return {
                    "success": True,
                    "text": result_text,
                    "language": language,
                    "confidence": 1.0,  # DashScopeä¸æä¾›ç½®ä¿¡åº¦
                    "duration": 0  # æš‚æ— æŒç»­æ—¶é—´ä¿¡æ¯
                }
            else:
                return {
                    "success": False,
                    "error": f"ASR APIè°ƒç”¨å¤±è´¥: {response.message}",
                    "text": ""
                }
                
        except Exception as e:
            print(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": ""
            }
    
    async def text_to_speech(
        self, 
        text: str, 
        voice: str,  # ç§»é™¤é»˜è®¤å€¼ï¼Œå¼ºåˆ¶ä¼ å…¥éŸ³è‰²å‚æ•°
        speed: float = 1.0,
        stream: bool = False
    ) -> Union[bytes, AsyncGenerator[bytes, None]]:
        """
        æ–‡å­—è½¬è¯­éŸ³ (TTS)
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            voice: å£°éŸ³ç±»å‹
            speed: è¯­é€Ÿå€ç‡
            stream: æ˜¯å¦æµå¼è¿”å›
            
        Returns:
            éŸ³é¢‘æ•°æ®æˆ–éŸ³é¢‘æµ
        """
        try:
            if stream:
                return self._text_to_speech_stream(text, voice, speed)
            else:
                return await self._text_to_speech_batch(text, voice, speed)
                
        except Exception as e:
            print(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {e}")
            raise
    
    async def _generate_xiyang_voice(self, text: str, speed: float) -> bytes:
        """å–œç¾Šç¾Šä¸“ç”¨TTS - æ·±æ²‰ç”·å£°onyx"""
        print(f"ğŸ­ ç”Ÿæˆå–œç¾Šç¾Šå£°éŸ³ - voice=onyx, text={text[:20]}...")
        response = self.openai_client.audio.speech.create(
            model=Config.TTS_MODEL,
            voice="onyx",  # å›ºå®šä½¿ç”¨onyxæ·±æ²‰ç”·å£°
            input=text,
            speed=speed
        )
        audio_data = response.content
        print(f"âœ… å–œç¾Šç¾ŠTTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
        return audio_data
    
    async def _generate_meiyang_voice(self, text: str, speed: float) -> bytes:
        """ç¾ç¾Šç¾Šä¸“ç”¨TTS - ä¼˜é›…å¥³å£°nova"""
        print(f"ğŸŒ¸ ç”Ÿæˆç¾ç¾Šç¾Šå£°éŸ³ - voice=nova, text={text[:20]}...")
        response = self.openai_client.audio.speech.create(
            model=Config.TTS_MODEL,
            voice="nova",  # å›ºå®šä½¿ç”¨novaä¼˜é›…å¥³å£°
            input=text,
            speed=speed
        )
        audio_data = response.content
        print(f"âœ… ç¾ç¾Šç¾ŠTTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
        return audio_data
    
    async def _generate_lanyang_voice(self, text: str, speed: float) -> bytes:
        """æ‡’ç¾Šç¾Šä¸“ç”¨TTS - è‹±å›½å£éŸ³fable"""
        print(f"ğŸ‡¬ğŸ‡§ ç”Ÿæˆæ‡’ç¾Šç¾Šå£°éŸ³ - voice=fable, text={text[:20]}...")
        response = self.openai_client.audio.speech.create(
            model=Config.TTS_MODEL,
            voice="fable",  # å›ºå®šä½¿ç”¨fableè‹±å›½å£éŸ³
            input=text,
            speed=speed
        )
        audio_data = response.content
        print(f"âœ… æ‡’ç¾Šç¾ŠTTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
        return audio_data

    async def generate_character_voice(self, character_id: str, text: str, speed: float = 1.0) -> bytes:
        """æ ¹æ®è§’è‰²IDç”Ÿæˆä¸“ç”¨å£°éŸ³"""
        print(f"ğŸ¯ æ ¹æ®è§’è‰²IDé€‰æ‹©ä¸“ç”¨TTS - character_id={character_id}")
        
        if character_id == "xiyang":
            return await self._generate_xiyang_voice(text, speed)
        elif character_id == "meiyang":
            return await self._generate_meiyang_voice(text, speed)
        elif character_id == "lanyang":
            return await self._generate_lanyang_voice(text, speed)
        else:
            # é»˜è®¤ä½¿ç”¨onyxå£°éŸ³
            print(f"âš ï¸ æœªçŸ¥è§’è‰²ID {character_id}ï¼Œä½¿ç”¨é»˜è®¤onyxå£°éŸ³")
            return await self._generate_xiyang_voice(text, speed)

    async def _text_to_speech_batch(
        self, 
        text: str, 
        voice: str, 
        speed: float
    ) -> bytes:
        """æ‰¹é‡TTSå¤„ç† - ä½¿ç”¨è§’è‰²ä¸“ç”¨TTSå‡½æ•°"""
        try:
            # å¦‚æœå¯ç”¨äº†OpenAI TTSï¼Œæ ¹æ®voiceå‚æ•°é€‰æ‹©ä¸“ç”¨å‡½æ•°
            if Config.USE_OPENAI_TTS:
                print(f"ğŸµ é€‰æ‹©è§’è‰²ä¸“ç”¨TTSå‡½æ•° - voice={voice}")
                
                # æ ¹æ®voiceå‚æ•°è°ƒç”¨å¯¹åº”çš„ä¸“ç”¨å‡½æ•°
                if voice == "onyx":
                    return await self._generate_xiyang_voice(text, speed)
                elif voice == "nova":  # ç¾ç¾Šç¾Šç°åœ¨ä½¿ç”¨novaéŸ³è‰²
                    return await self._generate_meiyang_voice(text, speed)
                elif voice == "fable":
                    return await self._generate_lanyang_voice(text, speed)
                else:
                    # é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨é€šç”¨æ–¹æ³•
                    print(f"ğŸµ ä½¿ç”¨é€šç”¨OpenAI TTS - model={Config.TTS_MODEL}, voice={voice}, text={text[:20]}...")
                    response = self.openai_client.audio.speech.create(
                        model=Config.TTS_MODEL,
                        voice=voice,
                        input=text,
                        speed=speed
                    )
                    audio_data = response.content
                    print(f"âœ… é€šç”¨OpenAI TTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
                    return audio_data
            
            # å¦‚æœä½¿ç”¨DashScopeï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ä»½ï¼‰
            else:
                import dashscope
                from dashscope.audio.tts import SpeechSynthesizer
                
                # è®¾ç½®API key
                dashscope.api_key = self.api_key
                
                response = SpeechSynthesizer.call(
                    model=Config.TTS_MODEL,
                    text=text,
                    voice=voice,
                    sample_rate=24000,
                    format='wav'
                )
                
                print(f"ğŸµ DashScope TTSè°ƒç”¨å‚æ•°: model={Config.TTS_MODEL}, voice={voice}, text={text[:20]}...")
                
                # DashScope SDKè¿”å›çš„æ˜¯SpeechSynthesisResultå¯¹è±¡ï¼Œä¸æ˜¯HTTPå“åº”
                if hasattr(response, 'get_audio_data'):
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    api_response = response.get_response()
                    print(f"ğŸ” APIå“åº”: {api_response}")
                    
                    # ç›´æ¥è·å–éŸ³é¢‘æ•°æ®
                    audio_data = response.get_audio_data()
                    if audio_data:
                        print(f"âœ… DashScope TTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
                        return audio_data
                    else:
                        print("âš ï¸ å“åº”æˆåŠŸä½†æ— éŸ³é¢‘æ•°æ®")
                else:
                    # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    print(f"ğŸ” TTSå“åº”ç»“æ„: {type(response)}")
                    print(f"ğŸ” responseå±æ€§: {dir(response)}")
                    
                    if hasattr(response, 'output'):
                        print(f"ğŸ” outputç±»å‹: {type(response.output)}")
                        print(f"ğŸ” outputå±æ€§: {dir(response.output)}") 
                    
                        if hasattr(response.output, 'audio'):
                            print(f"ğŸ” audioç±»å‹: {type(response.output.audio)}")
                            print(f"ğŸ” audioå†…å®¹: {response.output.audio}")
                        
                        # å°è¯•ä¸åŒçš„è®¿é—®æ–¹å¼
                        if isinstance(response.output.audio, dict):
                            if 'url' in response.output.audio:
                                audio_url = response.output.audio['url']
                                print(f"âœ… ä½¿ç”¨å­—å…¸æ–¹å¼è·å¾—URL: {audio_url}")
                                
                                # ä¸‹è½½éŸ³é¢‘æ•°æ®
                                import requests
                                audio_response = requests.get(audio_url)
                                audio_response.raise_for_status()
                                
                                audio_data = audio_response.content
                                print(f"âœ… éŸ³é¢‘ä¸‹è½½æˆåŠŸ: {len(audio_data)}å­—èŠ‚")
                                return audio_data
                            
                        elif 'data' in response.output.audio:
                            # ç›´æ¥è¿”å›Base64æ•°æ®
                            audio_data = base64.b64decode(response.output.audio['data'])
                            print(f"âœ… ä½¿ç”¨Base64æ•°æ®: {len(audio_data)}å­—èŠ‚")
                            return audio_data
                    
                    elif hasattr(response.output.audio, 'url'):
                        audio_url = response.output.audio.url
                        print(f"âœ… ä½¿ç”¨å±æ€§æ–¹å¼è·å¾—URL: {audio_url}")
                        
                        # ä¸‹è½½éŸ³é¢‘æ•°æ®
                        audio_response = requests.get(audio_url)
                        audio_response.raise_for_status()
                        
                        audio_data = audio_response.content
                        print(f"âœ… éŸ³é¢‘ä¸‹è½½æˆåŠŸ: {len(audio_data)}å­—èŠ‚")
                        return audio_data
            
            raise Exception(f"TTSå“åº”æ ¼å¼é”™è¯¯: æ— æ³•è§£æéŸ³é¢‘æ•°æ® - å“åº”: {response}")
                
        except Exception as e:
            print(f"âŒ TTSå¤„ç†å¤±è´¥: {e}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e)}")
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")
            raise
    
    async def _text_to_speech_stream(
        self, 
        text: str, 
        voice: str, 
        speed: float
    ) -> AsyncGenerator[bytes, None]:
        """æµå¼TTSå¤„ç† - ä½¿ç”¨qwen3-tts-flash-realtimeæ¨¡å‹"""
        try:
            # æµå¼æ¨¡å¼ä½¿ç”¨SpeechSynthesizer
            from dashscope.audio.qwen_tts import SpeechSynthesizer
            
            response = SpeechSynthesizer.call(
                model=Config.TTS_MODEL,
                api_key=self.api_key,
                text=text,
                voice=voice,
                language_type="Chinese",
                stream=True
            )
            
            for chunk in response:
                if (hasattr(chunk, 'output') and 
                    hasattr(chunk.output, 'audio') and 
                    chunk.output.audio is not None):
                    
                    if hasattr(chunk.output.audio, 'data'):
                        # æµå¼æ•°æ®ç›´æ¥è¿”å›
                        audio_data = base64.b64decode(chunk.output.audio.data)
                        yield audio_data
                    elif hasattr(chunk.output.audio, 'url'):
                        # å¦‚æœæµå¼è¿”å›URLï¼Œä¸‹è½½å¹¶è¿”å›
                        audio_url = chunk.output.audio.url
                        audio_response = requests.get(audio_url)
                        audio_response.raise_for_status()
                        yield audio_response.content
                    
                if (hasattr(chunk, 'output') and 
                    hasattr(chunk.output, 'finish_reason') and 
                    chunk.output.finish_reason == "stop"):
                    break
                    
        except Exception as e:
            print(f"âŒ æµå¼TTSå¤„ç†å¤±è´¥: {e}")
            raise
    
    def save_audio_to_file(
        self, 
        audio_data: bytes, 
        file_path: str, 
        audio_format: str = 'wav'
    ):
        """
        ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„
            audio_format: éŸ³é¢‘æ ¼å¼
        """
        try:
            with open(file_path, 'wb') as f:
                f.write(audio_data)
            
            print(f"âœ… éŸ³é¢‘å·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def load_audio_from_file(self, file_path: str) -> bytes:
        """
        ä»æ–‡ä»¶åŠ è½½éŸ³é¢‘
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ•°æ®
        """
        try:
            with open(file_path, 'rb') as f:
                return f.read()
                
        except Exception as e:
            print(f"âŒ åŠ è½½éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def get_audio_info(self, audio_data: bytes, audio_format: str = 'wav') -> Dict[str, Any]:
        """
        è·å–éŸ³é¢‘ä¿¡æ¯
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            audio_format: éŸ³é¢‘æ ¼å¼
            
        Returns:
            éŸ³é¢‘ä¿¡æ¯
        """
        try:
            audio = AudioSegment.from_file(
                io.BytesIO(audio_data), 
                format=audio_format
            )
            
            return {
                "duration": len(audio) / 1000.0,  # ç§’
                "frame_rate": audio.frame_rate,
                "channels": audio.channels,
                "sample_width": audio.sample_width,
                "frame_count": audio.frame_count(),
                "format": audio_format
            }
            
        except Exception as e:
            print(f"âŒ è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def test_audio_pipeline(self, test_text: str = "ä½ å¥½ï¼Œæˆ‘æ˜¯FamilyBotæµ‹è¯•è¯­éŸ³ã€‚") -> bool:
        """
        æµ‹è¯•éŸ³é¢‘å¤„ç†ç®¡é“
        
        Args:
            test_text: æµ‹è¯•æ–‡æœ¬
            
        Returns:
            æµ‹è¯•æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ§ª å¼€å§‹æµ‹è¯•éŸ³é¢‘ç®¡é“...")
            
            # æµ‹è¯•TTS
            print(f"ğŸ“¢ æµ‹è¯•TTS: {test_text}")
            audio_data = await self.text_to_speech(test_text)
            
            if not audio_data:
                print("âŒ TTSæµ‹è¯•å¤±è´¥")
                return False
            
            print(f"âœ… TTSæˆåŠŸï¼Œç”Ÿæˆ {len(audio_data)} å­—èŠ‚éŸ³é¢‘æ•°æ®")
            
            # è·å–éŸ³é¢‘ä¿¡æ¯
            audio_info = self.get_audio_info(audio_data)
            print(f"ğŸµ éŸ³é¢‘ä¿¡æ¯: {audio_info}")
            
            # æµ‹è¯•ASRï¼ˆä½¿ç”¨ç”Ÿæˆçš„éŸ³é¢‘ï¼‰
            print(f"ğŸ™ï¸ æµ‹è¯•ASR...")
            asr_result = await self.speech_to_text(audio_data)
            
            if asr_result["success"]:
                print(f"âœ… ASRæˆåŠŸ: {asr_result['text']}")
                return True
            else:
                print(f"âŒ ASRå¤±è´¥: {asr_result['error']}")
                return False
                
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç®¡é“æµ‹è¯•å¤±è´¥: {e}")
            return False


# å…¨å±€éŸ³é¢‘æœåŠ¡å®ä¾‹
audio_service = AudioService()
