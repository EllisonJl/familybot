import os
from dotenv import load_dotenv
from openai import OpenAI

# DashScope è¯­éŸ³æ¨¡å‹
from dashscope.audio.tts import SpeechSynthesizer
from dashscope.audio.asr import Transcription

# === åŠ è½½ .env æ–‡ä»¶ ===
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '.env'))
load_dotenv(dotenv_path=dotenv_path, override=True)

# è·å–ç¯å¢ƒå˜é‡
api_key = os.getenv("DASHSCOPE_API_KEY")
base_url = os.getenv("DASHSCOPE_BASE_URL")

if not api_key:
    raise RuntimeError("âŒ æœªè®¾ç½® DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼")

print(f"âœ… å½“å‰ä½¿ç”¨çš„ API KEY: {repr(api_key)}")

# åˆ›å»º DashScope/OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    api_key=api_key,
    base_url=base_url
)

# === æµ‹è¯• LLM ===
print("\n=== ğŸ§  æµ‹è¯• Qwen LLM ===")
response = client.chat.completions.create(
    model="qwen3-max-preview",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]
)
print("ğŸ¤– LLM å›å¤:", response.choices[0].message.content)


import dashscope

# âœ… ç›´æ¥å†™å…¥ä½ æµ‹è¯•æˆåŠŸçš„ API Key
api_key = "sk-5c523ce367f3486e9bd68f87be0fcabb"

print(f"âœ… å½“å‰ä½¿ç”¨çš„ API KEY: {repr(api_key)}")

# === æ„å»º ASR è¯·æ±‚ ===
messages = [
    {
        "role": "system",
        "content": [{"text": ""}]
    },
    {
        "role": "user",
        "content": [
            {"audio": "https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3"},
        ]
    }
]

response = dashscope.MultiModalConversation.call(
    api_key=api_key,
    model="qwen3-asr-flash",
    messages=messages,
    result_format="message",
    asr_options={
        "enable_lid": True,
        "enable_itn": False
    }
)

print("\nğŸ§  ASR è¯†åˆ«ç»“æœ:")
print(response)


import os
import requests
import dashscope
import pyaudio
import time
import base64
import numpy as np
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '.env'))
load_dotenv(dotenv_path=dotenv_path, override=True)

# DashScope è¯­éŸ³æ¨¡å‹
from dashscope.audio.tts import SpeechSynthesizer

api_key = os.getenv("DASHSCOPE_API_KEY")

p = pyaudio.PyAudio()

stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=24000,
                output=True)

text = "ä½ å¥½å•Šï¼Œæˆ‘æ˜¯é€šä¹‰åƒé—®"
response = dashscope.MultiModalConversation.call(
    api_key=api_key,
    model="qwen3-tts-flash",
    text=text,
    voice="Cherry",
    language_type="Chinese",
    stream=True
)

for chunk in response:
    # æ‰“å°æ¯ä¸ªå—çš„å†…å®¹ï¼Œä»¥è¿›è¡Œè°ƒè¯•
    print(f"Received chunk: {chunk}")

    if chunk.output is not None and chunk.output.audio is not None:
        audio = chunk.output.audio
        wav_bytes = base64.b64decode(audio.data)
        audio_np = np.frombuffer(wav_bytes, dtype=np.int16)
        # ç›´æ¥æ’­æ”¾éŸ³é¢‘æ•°æ®
        stream.write(audio_np.tobytes())
    if chunk.output is not None and chunk.output.finish_reason == "stop":
        print("finish at: {} ", chunk.output.audio.expires_at)

time.sleep(0.8)

stream.stop_stream()
stream.close()
p.terminate()
