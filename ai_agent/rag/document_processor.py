"""
æ–‡æ¡£å¤„ç†å™¨
æ”¯æŒPDFã€Wordã€TXTç­‰æ ¼å¼æ–‡æ¡£çš„è§£æå’ŒçŸ¥è¯†æå–
"""

import os
import json
import hashlib
import sqlite3
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
import uuid

# æ–‡æ¡£è§£æåº“
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

from openai import OpenAI
from config import Config


@dataclass
class DocumentMetadata:
    """æ–‡æ¡£å…ƒæ•°æ®"""
    file_id: str
    filename: str
    file_path: str
    file_size: int
    file_type: str
    upload_time: str
    character_id: str
    user_id: str
    content_hash: str
    page_count: int = 0
    word_count: int = 0
    summary: str = ""
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []


@dataclass
class DocumentChunk:
    """æ–‡æ¡£ç‰‡æ®µ"""
    chunk_id: str
    file_id: str
    content: str
    chunk_index: int
    page_number: int = 0
    chunk_type: str = "text"  # text, table, image
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, db_path: Optional[str] = None, storage_path: Optional[str] = None):
        """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨"""
        if db_path is None:
            db_dir = Path(__file__).parent.parent / "data"
            db_dir.mkdir(exist_ok=True)
            db_path = str(db_dir / "document_storage.db")
            
        if storage_path is None:
            storage_path = str(Path(__file__).parent.parent / "data" / "uploaded_files")
            
        self.db_path = db_path
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        
        self.client = OpenAI(
            api_key=Config.DASHSCOPE_API_KEY,
            base_url=Config.DASHSCOPE_BASE_URL
        )
        
        self._init_database()
        print("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ–‡æ¡£å­˜å‚¨æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡æ¡£å…ƒæ•°æ®è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_metadata (
                    file_id TEXT PRIMARY KEY,
                    filename TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    file_size INTEGER NOT NULL,
                    file_type TEXT NOT NULL,
                    upload_time TEXT NOT NULL,
                    character_id TEXT NOT NULL,
                    user_id TEXT NOT NULL,
                    content_hash TEXT NOT NULL,
                    page_count INTEGER DEFAULT 0,
                    word_count INTEGER DEFAULT 0,
                    summary TEXT DEFAULT '',
                    keywords TEXT DEFAULT '[]'
                )
            """)
            
            # æ–‡æ¡£ç‰‡æ®µè¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_chunks (
                    chunk_id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    content TEXT NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    page_number INTEGER DEFAULT 0,
                    chunk_type TEXT DEFAULT 'text',
                    metadata TEXT DEFAULT '{}',
                    FOREIGN KEY (file_id) REFERENCES document_metadata (file_id)
                )
            """)
            
            # è§’è‰²æ–‡ä»¶å…³è”è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS character_files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    character_id TEXT NOT NULL,
                    file_id TEXT NOT NULL,
                    access_time TEXT NOT NULL,
                    FOREIGN KEY (file_id) REFERENCES document_metadata (file_id),
                    UNIQUE(character_id, file_id)
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_metadata_character 
                ON document_metadata(character_id)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_chunks_file 
                ON document_chunks(file_id)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_character_files_character 
                ON character_files(character_id)
            """)
            
            conn.commit()
    
    async def upload_file(
        self, 
        file_content: bytes, 
        filename: str, 
        character_id: str, 
        user_id: str
    ) -> Tuple[bool, str, Optional[DocumentMetadata]]:
        """
        ä¸Šä¼ æ–‡ä»¶å¹¶å¤„ç†
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            character_id: è§’è‰²ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            (æˆåŠŸæ ‡å¿—, æ¶ˆæ¯, æ–‡æ¡£å…ƒæ•°æ®)
        """
        try:
            print(f"ğŸ“¤ å¼€å§‹å¤„ç†æ–‡ä»¶ä¸Šä¼ : {filename}")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            file_type = self._get_file_type(filename)
            if not self._is_supported_file_type(file_type):
                return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}", None
            
            # ç”Ÿæˆæ–‡ä»¶IDå’Œè·¯å¾„
            file_id = str(uuid.uuid4())
            content_hash = hashlib.md5(file_content).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è¿‡ç›¸åŒæ–‡ä»¶
            existing_file = self._check_duplicate_file(content_hash, character_id)
            if existing_file:
                return False, "è¯¥è§’è‰²å·²ä¸Šä¼ è¿‡ç›¸åŒå†…å®¹çš„æ–‡ä»¶", existing_file
            
            # ä¿å­˜æ–‡ä»¶
            file_path = self._save_file(file_content, file_id, filename)
            
            # è§£ææ–‡æ¡£å†…å®¹
            chunks = await self._parse_document(file_path, file_id, file_type)
            print(f"ğŸ“– æ–‡æ¡£è§£æç»“æœ: æ‰¾åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")
            if not chunks:
                return False, f"æ–‡æ¡£è§£æå¤±è´¥ï¼šæ–‡ä»¶ {filename} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½æ˜¯ç©ºæ–‡ä»¶ã€åŠ å¯†æ–‡ä»¶æˆ–æ ¼å¼é”™è¯¯", None
            
            # ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å’Œå…³é”®è¯
            summary, keywords = await self._generate_document_summary(chunks)
            
            # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®
            metadata = DocumentMetadata(
                file_id=file_id,
                filename=filename,
                file_path=file_path,
                file_size=len(file_content),
                file_type=file_type,
                upload_time=datetime.now().isoformat(),
                character_id=character_id,
                user_id=user_id,
                content_hash=content_hash,
                page_count=max([chunk.page_number for chunk in chunks]) if chunks else 0,
                word_count=sum([len(chunk.content) for chunk in chunks]),
                summary=summary,
                keywords=keywords
            )
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            self._store_document_metadata(metadata)
            self._store_document_chunks(chunks)
            self._add_character_file_relation(character_id, file_id)
            
            print(f"âœ… æ–‡ä»¶ä¸Šä¼ å¤„ç†å®Œæˆ: {filename}, ID: {file_id}")
            return True, "æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†æˆåŠŸ", metadata
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
            return False, f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}", None
    
    def _get_file_type(self, filename: str) -> str:
        """è·å–æ–‡ä»¶ç±»å‹"""
        return Path(filename).suffix.lower()
    
    def _is_supported_file_type(self, file_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
        supported_types = ['.txt', '.md']
        
        if PDF_AVAILABLE:
            supported_types.extend(['.pdf'])
            
        if DOCX_AVAILABLE:
            supported_types.extend(['.docx', '.doc'])
            
        return file_type in supported_types
    
    def _check_duplicate_file(self, content_hash: str, character_id: str) -> Optional[DocumentMetadata]:
        """æ£€æŸ¥é‡å¤æ–‡ä»¶"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT * FROM document_metadata 
                    WHERE content_hash = ? AND character_id = ?
                """, (content_hash, character_id))
                
                row = cursor.fetchone()
                if row:
                    return DocumentMetadata(
                        file_id=row[0],
                        filename=row[1],
                        file_path=row[2],
                        file_size=row[3],
                        file_type=row[4],
                        upload_time=row[5],
                        character_id=row[6],
                        user_id=row[7],
                        content_hash=row[8],
                        page_count=row[9],
                        word_count=row[10],
                        summary=row[11],
                        keywords=json.loads(row[12])
                    )
                return None
        except Exception as e:
            print(f"âŒ æ£€æŸ¥é‡å¤æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _save_file(self, file_content: bytes, file_id: str, filename: str) -> str:
        """ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°å­˜å‚¨"""
        file_extension = Path(filename).suffix
        safe_filename = f"{file_id}{file_extension}"
        file_path = self.storage_path / safe_filename
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
            
        return str(file_path)
    
    async def _parse_document(self, file_path: str, file_id: str, file_type: str) -> List[DocumentChunk]:
        """è§£ææ–‡æ¡£å†…å®¹"""
        try:
            if file_type == '.txt' or file_type == '.md':
                return self._parse_text_file(file_path, file_id)
            elif file_type == '.pdf' and PDF_AVAILABLE:
                return self._parse_pdf_file(file_path, file_id)
            elif file_type in ['.docx', '.doc'] and DOCX_AVAILABLE:
                return self._parse_docx_file(file_path, file_id)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                return []
                
        except Exception as e:
            print(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {e}")
            return []
    
    def _parse_text_file(self, file_path: str, file_id: str) -> List[DocumentChunk]:
        """è§£ææ–‡æœ¬æ–‡ä»¶"""
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            for i, paragraph in enumerate(paragraphs):
                if len(paragraph) > 20:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    chunk = DocumentChunk(
                        chunk_id=f"{file_id}_chunk_{i}",
                        file_id=file_id,
                        content=paragraph,
                        chunk_index=i,
                        page_number=1,
                        chunk_type="text"
                    )
                    chunks.append(chunk)
                    
            return chunks
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶è§£æå¤±è´¥: {e}")
            return []
    
    def _parse_pdf_file(self, file_path: str, file_id: str) -> List[DocumentChunk]:
        """è§£æPDFæ–‡ä»¶"""
        chunks = []
        
        try:
            print(f"ğŸ“– å¼€å§‹è§£æPDFæ–‡ä»¶: {file_path}")
            with pdfplumber.open(file_path) as pdf:
                print(f"ğŸ“„ PDFé¡µæ•°: {len(pdf.pages)}")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    print(f"ğŸ“„ ç¬¬{page_num}é¡µæå–çš„æ–‡æœ¬é•¿åº¦: {len(text) if text else 0}")
                    
                    if text and text.strip():
                        # æŒ‰æ®µè½åˆ†å‰²
                        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                        print(f"ğŸ“„ ç¬¬{page_num}é¡µåˆ†å‰²å‡º {len(paragraphs)} ä¸ªæ®µè½")
                        
                        for i, paragraph in enumerate(paragraphs):
                            # é™ä½æœ€å°é•¿åº¦è¦æ±‚åˆ°20ä¸ªå­—ç¬¦
                            if len(paragraph) > 20:
                                chunk = DocumentChunk(
                                    chunk_id=f"{file_id}_page_{page_num}_chunk_{i}",
                                    file_id=file_id,
                                    content=paragraph,
                                    chunk_index=len(chunks),
                                    page_number=page_num,
                                    chunk_type="text"
                                )
                                chunks.append(chunk)
                                print(f"âœ… æ·»åŠ æ–‡æœ¬å— {len(chunks)}: {paragraph[:50]}...")
                                
            print(f"ğŸ“– PDFè§£æå®Œæˆï¼Œå…±æå– {len(chunks)} ä¸ªæ–‡æœ¬å—")
            return chunks
            
        except Exception as e:
            print(f"âŒ PDFæ–‡ä»¶è§£æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _parse_docx_file(self, file_path: str, file_id: str) -> List[DocumentChunk]:
        """è§£æWordæ–‡æ¡£"""
        chunks = []
        
        try:
            doc = DocxDocument(file_path)
            
            for i, paragraph in enumerate(doc.paragraphs):
                text = paragraph.text.strip()
                if text and len(text) > 20:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    chunk = DocumentChunk(
                        chunk_id=f"{file_id}_para_{i}",
                        file_id=file_id,
                        content=text,
                        chunk_index=i,
                        page_number=1,  # Wordæ–‡æ¡£æ²¡æœ‰æ˜ç¡®çš„é¡µé¢æ¦‚å¿µ
                        chunk_type="text"
                    )
                    chunks.append(chunk)
                    
            return chunks
            
        except Exception as e:
            print(f"âŒ Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
            return []
    
    async def _generate_document_summary(self, chunks: List[DocumentChunk]) -> Tuple[str, List[str]]:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å’Œå…³é”®è¯"""
        try:
            # å–å‰å‡ ä¸ªchunkä½œä¸ºæ ·æœ¬
            sample_content = "\n\n".join([chunk.content for chunk in chunks[:5]])
            
            summary_prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š

{sample_content[:1000]}

æ‘˜è¦åº”è¯¥åŒ…å«ï¼š
1. æ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜
2. å…³é”®ä¿¡æ¯ç‚¹
3. æ–‡æ¡£ç±»å‹ï¼ˆå¦‚ï¼šåŒ»å­¦æŠ¥å‘Šã€æŠ€æœ¯æ–‡æ¡£ã€ä¸ªäººè®°å½•ç­‰ï¼‰

åªè¿”å›æ‘˜è¦å†…å®¹ï¼š
"""
            
            response = self.client.chat.completions.create(
                model=Config.LLM_MODEL,
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.3,
                max_tokens=300
            )
            
            summary = response.choices[0].message.content.strip()
            
            # ç”Ÿæˆå…³é”®è¯
            keywords_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæå–5-10ä¸ªå…³é”®è¯ï¼š

{sample_content[:1000]}

åªè¿”å›å…³é”®è¯åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼š
"""
            
            response = self.client.chat.completions.create(
                model=Config.LLM_MODEL,
                messages=[{"role": "user", "content": keywords_prompt}],
                temperature=0.3,
                max_tokens=200
            )
            
            keywords_text = response.choices[0].message.content.strip()
            keywords = [kw.strip('- ').strip() for kw in keywords_text.split('\n') if kw.strip()]
            
            return summary, keywords[:10]
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
            return "æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¤±è´¥", []
    
    def _store_document_metadata(self, metadata: DocumentMetadata):
        """å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO document_metadata 
                (file_id, filename, file_path, file_size, file_type, upload_time, 
                 character_id, user_id, content_hash, page_count, word_count, summary, keywords)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                metadata.file_id, metadata.filename, metadata.file_path,
                metadata.file_size, metadata.file_type, metadata.upload_time,
                metadata.character_id, metadata.user_id, metadata.content_hash,
                metadata.page_count, metadata.word_count, metadata.summary,
                json.dumps(metadata.keywords)
            ))
            conn.commit()
    
    def _store_document_chunks(self, chunks: List[DocumentChunk]):
        """å­˜å‚¨æ–‡æ¡£ç‰‡æ®µ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            chunk_data = [
                (
                    chunk.chunk_id, chunk.file_id, chunk.content,
                    chunk.chunk_index, chunk.page_number, chunk.chunk_type,
                    json.dumps(chunk.metadata)
                )
                for chunk in chunks
            ]
            
            cursor.executemany("""
                INSERT OR REPLACE INTO document_chunks 
                (chunk_id, file_id, content, chunk_index, page_number, chunk_type, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, chunk_data)
            
            conn.commit()
    
    def _add_character_file_relation(self, character_id: str, file_id: str):
        """æ·»åŠ è§’è‰²æ–‡ä»¶å…³è”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO character_files 
                (character_id, file_id, access_time)
                VALUES (?, ?, ?)
            """, (character_id, file_id, datetime.now().isoformat()))
            conn.commit()
    
    def get_character_files(self, character_id: str) -> List[DocumentMetadata]:
        """è·å–è§’è‰²çš„æ‰€æœ‰æ–‡ä»¶"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT dm.* FROM document_metadata dm
                    JOIN character_files cf ON dm.file_id = cf.file_id
                    WHERE cf.character_id = ?
                    ORDER BY dm.upload_time DESC
                """, (character_id,))
                
                rows = cursor.fetchall()
                files = []
                
                for row in rows:
                    files.append(DocumentMetadata(
                        file_id=row[0],
                        filename=row[1],
                        file_path=row[2],
                        file_size=row[3],
                        file_type=row[4],
                        upload_time=row[5],
                        character_id=row[6],
                        user_id=row[7],
                        content_hash=row[8],
                        page_count=row[9],
                        word_count=row[10],
                        summary=row[11],
                        keywords=json.loads(row[12])
                    ))
                
                return files
                
        except Exception as e:
            print(f"âŒ è·å–è§’è‰²æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    async def search_in_character_documents(
        self, 
        character_id: str, 
        query: str, 
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """åœ¨è§’è‰²çš„æ–‡æ¡£ä¸­æœç´¢ç›¸å…³å†…å®¹"""
        try:
            print(f"ğŸ” æœç´¢æ–‡æ¡£ - è§’è‰²: {character_id}, æŸ¥è¯¢: '{query}', é™åˆ¶: {limit}")
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¯¥è§’è‰²çš„æ–‡æ¡£
                cursor.execute("""
                    SELECT COUNT(*) FROM character_files WHERE character_id = ?
                """, (character_id,))
                file_count = cursor.fetchone()[0]
                print(f"ğŸ“š è§’è‰² {character_id} æ‹¥æœ‰ {file_count} ä¸ªæ–‡æ¡£")
                
                # æ™ºèƒ½æå–å…³é”®è¯è¿›è¡Œæœç´¢
                import re
                
                # æ‰‹åŠ¨åˆ†å‰²ä¸­æ–‡çŸ­è¯­å’Œè¯ç»„
                common_patterns = [
                    r'çˆ†ç±³èŠ±é•‡',
                    r'å¤§éªšåŠ¨', 
                    r'å˜å˜Ÿ',
                    r'ç‰ç±³ç²¾çµ',
                    r'ç«ç§çŸ³',
                    r'çš®çš®çŒ´',
                    r'èƒ–å¢©ç‹—'
                ]
                
                # æŸ¥æ‰¾å·²çŸ¥çš„é‡è¦è¯ç»„
                search_terms = []
                for pattern in common_patterns:
                    if re.search(pattern, query):
                        search_terms.append(pattern)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šè¯ç»„ï¼Œä½¿ç”¨åŸºæœ¬çš„è¯æ±‡åˆ†å‰²
                if not search_terms:
                    # æå–2-6ä¸ªå­—ç¬¦çš„ä¸­æ–‡è¯ç»„
                    words = []
                    for i in range(len(query)):
                        for j in range(i+2, min(i+7, len(query)+1)):
                            word = query[i:j]
                            if re.match(r'^[\u4e00-\u9fff]+$', word):
                                words.append(word)
                    
                    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
                    stop_words = {'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å†…å®¹', 'è¯´äº†', 'æ•…äº‹', 'è¿™æ˜¯', 'é‚£æ˜¯'}
                    search_terms = [word for word in words if word not in stop_words and len(word) >= 2]
                    
                    # å»é‡å¹¶ä¿æŒé¡ºåº
                    seen = set()
                    search_terms = [word for word in search_terms if not (word in seen or seen.add(word))]
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                if not search_terms:
                    search_terms = [query]
                
                print(f"ğŸ” æå–çš„å…³é”®è¯: {search_terms[:5]}")  # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ª
                
                all_results = []
                
                # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæœç´¢
                for term in search_terms:
                    cursor.execute("""
                        SELECT dc.content, dc.page_number, dm.filename, dm.file_id
                        FROM document_chunks dc
                        JOIN document_metadata dm ON dc.file_id = dm.file_id
                        JOIN character_files cf ON dm.file_id = cf.file_id
                        WHERE cf.character_id = ? AND dc.content LIKE ?
                        ORDER BY dm.upload_time DESC
                        LIMIT ?
                    """, (character_id, f"%{term}%", limit))
                    
                    rows = cursor.fetchall()
                    print(f"ğŸ” è¯æ±‡ '{term}' æ‰¾åˆ° {len(rows)} ä¸ªç»“æœ")
                    
                    for row in rows:
                        result = {
                            "content": row[0],
                            "page_number": row[1],
                            "filename": row[2],
                            "file_id": row[3],
                            "relevance_score": 0.8  # ç®€åŒ–çš„ç›¸å…³æ€§åˆ†æ•°
                        }
                        # é¿å…é‡å¤ç»“æœ
                        if not any(r["content"] == result["content"] for r in all_results):
                            all_results.append(result)
                
                print(f"ğŸ“„ æ€»å…±æ‰¾åˆ° {len(all_results)} ä¸ªå”¯ä¸€ç»“æœ")
                return all_results[:limit]
                
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def delete_character_file(self, character_id: str, file_id: str) -> bool:
        """åˆ é™¤è§’è‰²çš„æ–‡ä»¶"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºè¯¥è§’è‰²
                cursor.execute("""
                    SELECT file_path FROM document_metadata 
                    WHERE file_id = ? AND character_id = ?
                """, (file_id, character_id))
                
                row = cursor.fetchone()
                if not row:
                    return False
                
                file_path = row[0]
                
                # åˆ é™¤æ•°æ®åº“è®°å½•
                cursor.execute("DELETE FROM character_files WHERE character_id = ? AND file_id = ?", 
                             (character_id, file_id))
                cursor.execute("DELETE FROM document_chunks WHERE file_id = ?", (file_id,))
                cursor.execute("DELETE FROM document_metadata WHERE file_id = ?", (file_id,))
                
                # åˆ é™¤ç‰©ç†æ–‡ä»¶
                if os.path.exists(file_path):
                    os.remove(file_path)
                
                conn.commit()
                print(f"âœ… å·²åˆ é™¤æ–‡ä»¶: {file_id}")
                return True
                
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
            return False


# åˆ›å»ºå…¨å±€æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
document_processor = DocumentProcessor()
